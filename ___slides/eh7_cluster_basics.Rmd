---
title: "Clusteranalyse - Grundlegendes"
subtitle: "716408 | How 2 do Things with even more Numbers"
author: "KMH"
date: "WS 21-22 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [mycss_metropolis_v1.css, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r xaringanExtras, echo=FALSE}
xaringanExtra::use_share_again()  # f√ºr die Buttons zur Navigation
xaringanExtra::use_tile_view()    # f√ºr den Overview auf die Slides
```

# Wo wir gerade stehen

```{r echo=FALSE}
knitr::asis_output('<div class="container">')
knitr::include_graphics ("images/eh4_warmup2/ImageSlide_23_modellieren.png")
knitr::asis_output("</div>")
```

---

class: zwischentitel, center, middle

# .emolarge[ü§î]<br>Clusteranalyse?

---

# Warum und wozu Clusteranalyse?

**strukturentdeckende Verfahren:**  
‚Ü™ "√§hnliche*" Merkmalstr√§ger in Gruppen zusammenfassen

Einige **geographische Beispiele**:

-   um Aktivit√§ten zur Anpassung an den Klimawandel inhaltlich zu sortieren ([z.B. Gr√ºneis et al. 2018](https://doi.org/10.1016/j.landusepol.2018.08.025));
-   um Verhaltenstypen zur Vorsorge gegen√ºber Naturgefahren zu unterscheiden ([z.B. Posch et al. 2019](https://doi.org/10.1659/MRD-JOURNAL-D-19-00005.1));
-   oder um Typen l√§ndlicher R√§ume zu unterscheiden  
([z.B. H√∂ferl et al. 2007](http://www.agit.at/s_c/papers/2007/6228.pdf)).

---

# @ √Ñhnlichkeit

```{r echo=FALSE}
  knitr::asis_output('<div class="container">')
  knitr::include_graphics("images/eh7_cluster/aehnlichkeit.png")
  knitr::asis_output('</div>')
```

---

# Zielsetzung beim Bilden der Gruppen

A. **hohe Intracluster-Homogenit√§t**: √Ñhnliches in einen Cluster

B. **hohen Intercluster-Heterogenit√§t klar**:  
Cluster klar unterschiedlich

```{r echo=FALSE}
  knitr::asis_output('<div class="container400">')
  knitr::include_graphics("images/eh7_cluster/hetero_homogenitaet_noLegend.png")
  knitr::asis_output('</div>')
```

---

class: zwischentitel, center, middle

# .emolarge[ü§î]<br>Wie geht das?

---

# Ablauf einer Clusterung

**Zwei zentrale Arbeitsschritte:**

1.  Zuerst: √Ñhnlichkeit bzw. Distanz zwischen den zu gruppierenden Merkmalstr√§gern ermitteln

  + ‚Ü™ **Proximit√§tsma√üe**.

2.  Danach: Anhand eines **Gruppierungsverfahrens** gruppierbare (= die "√§hnlichsten") Merkmalstr√§ger zusammenfassen

  + ‚Ü™ **Fusionsalgorithmen**
  
üìö Mittlerweile gibt es Vielzahl clusteranalytischer Verfahren, die sich hinsichtlich dieser beiden Arbeitsschritte unterscheiden (vgl. Backhaus et al. 2017:438ff.).
---

class: zwischentitel, center, middle

# .emolarge[ü§î]<br>Welche Proximit√§tsma√üe gibt es?

---

# .font80[Ein kurzer √úberblick auf Proximit√§tsma√üe]

+ **Ziel:** Bestimmung der √Ñhnlichkeit von Merkmalstr√§gern

  * √Ñhnlichkeit = Indikator der je Merkmalstr√§ger √ºber all seine Merkmale hinweg ermittelt und aggregiert wird.
  
+ **Operationalisierung** von "√Ñhnlichkeit":

  * √Ñhnlichkeitsma√üe wie zB Korrelationen
  
  * Distanzma√üe wie zB euklidische Distanz
  
---

# Ausgew√§hlte √Ñhnlichkeitsma√üe

abh√§nging vom Skalenniveau:

| metrische Merkmale  | nominale Merkmale                 | bin√§re (0/1) Merkmale                          |
|:----|:----|:----|
| Kosinus             | Transformation in bin√§re Variable | W√ºrfelma√ü (Dice- oder Czekanowski-Koeffizient) |
| Pearson-Korrelation |                                   | Jaccard-Koeffizient                            |
|                     |                                   | M-Koeffizient (einfache √úbereinstimmung)       |
|                     |                                   | Kulczynski-Koeffizient                         |
|                     |                                   | Rogers und Tanimoto                            |
|                     |                                   | Russel & Rao (RR) Koeffizient                  |

---

# Ausgew√§hlte Distanzma√üe

abh√§nging vom Skalenniveau:

| metrische Merkmale          | nominale Merkmale | bin√§re (0/1) Merkmale         |
|:-----------------------------|:-------------------|:-------------------------------|
| (Quad.) Euklidische Distanz | Chi-Quadrat-Ma√ü   | Bin√§re Euklidische Distanz    |
| Minkowski Metrik            | Phi-Quadrat-Ma√ü   | Lance-Williams-Ma√ü            |
| Block Metrik                |                   | Bin√§re Form-Differenz         |
| Tschebyscheff Metrik        |                   | Gr√∂√üendifferenz               |
|                             |                   | Varianz                       |
|                             |                   | Rogers und Tanimoto           |
|                             |                   | Russel & Rao (RR) Koeffizient |

---

# Exkurs: ... and there is more

**Wie immer:** Packages bieten nat√ºrlich noch mehr √Ñhnlichkeits- und Distanzma√üe 

```{r}
library(philentropy)
getDistMethods()
```

---

# .font70[Wie entscheidet man sich f√ºr ein √Ñhnlichkeitsma√ü?]

* **üëâ Skalennievau** entscheidet
* danach: situationsabh√§ngig
* Ein (klassisches) Beispiel: **Euklidische Distanz** bei metrischen Variablen

```{r echo=FALSE}
  knitr::asis_output('<div class="container300">')
  knitr::include_graphics("images/eh7_cluster/eucledian_distance.jpg")
  knitr::asis_output('</div>')
```
.quelle[(Kmhkmh, Wikikmedia, CC BY)]

---

# Exkurs: Euklidische Distanz

F√ºr einen n-dimensionalen (n = Anzahl der Variablen) Fall kann die euklidische Distanz wie folgt ermittelt werden:

$$d_{(p,q)} = \sqrt{\sum_{i=1}^{n}(p_i - q_i)^2}$$

Zur Betonung von Unterschieden in den so ermittelten Distanzen:

**quadrierte euklidische Distanz** $d_{(p,q)}^2$

‚Ü™ Betonung der Unterschiedlichkeiten von Merkmalstr√§gern

---

class: zwischentitel, center, middle

# .emolarge[ü§î]<br>Welche Fusionsalgorithmen gibt es?

---

# .font80[Ein kurzer √úberblick auf Gruppierungsverfahren]

**Ziel:**  
Merkmalstr√§ger mit geringer Proximit√§t (hohe √Ñhnlichkeit bzw. geringe Distanz) in Gruppen zusammenfassen bzw. Grundgesamtheit in solche Gruppen zerteilen

```{r echo=FALSE}
  knitr::asis_output('<div class="container350">')
  knitr::include_graphics("images/eh7_cluster/clustervverfahren_uni_zuerich.jpg")
  knitr::asis_output('</div>')
```
.quelle[([Universit√§t Z√ºrich](https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html))]

---

# Der Klassiker:

**Hierarchisch agglomerative Verfahren**

* Beginn: jeder Merkmalstr√§ger = eigener Cluster
* Ermittlung der Proximit√§t
* Fusion der zwei "√§hnlichsten" Cluster
* Ermittlung der Proximit√§t
* Fusion ...
* ...
* Endergebnis: 1 Supercluster (= umfasst alle Merkmalstr√§ger)

---

# Der Klassiker graphisch gedacht:

```{r echo=FALSE}
  knitr::asis_output('<div class="container">')
  knitr::include_graphics("images/eh7_cluster/dendrogramm.png")
  knitr::asis_output('</div>')
```

---

# .font80[Fusionsalgorithmus &ne; Fusionsalgorithmus]

Das **Single Linkage (aka. "N√§chstgelegener Nachbar") Verfahren**:

Bei diesem Verfahren werden die beiden Cluster am √§hnlichsten bzw. n√§hesten zueinander eingestuft, deren Merkmalstr√§ger die geringste Distanz zueinander aufweisen:

```{r echo=FALSE}
  knitr::asis_output('<div class="container200">')
  knitr::include_graphics("images/eh7_cluster/single_linkage.jpg")
  knitr::asis_output('</div>')
```

  + Effekt 1: **Ausrei√üer** werden sichtbar
  + Effekt 2: "Kettenbildung" bedingt wenige, daf√ºr gro√üe Cluster
  
---

# .font80[Fusionsalgorithmus &ne; Fusionsalgorithmus]

Die **Ward Methode**:

Dieses Verfahren fokussiert nicht auf die Distanz von Clusterelementen zueinander, sondern auf die Varianz der Cluster. Ward definiert Varianz dabei als die Summe der quadrierten Abweichungen ("ESS - Error Sum of Squares") der Merkmalstr√§ger in einem Cluster zum Cluster-Mittelwert. 

  + ‚Ü™ jene zwei Cluster fusionieren, deren Fusion die **Varianz √ºber alle Cluster** am wenigsten erh√∂ht
  + **Effekt:** gleich gro√üe Cluster

```{r include=FALSE}
# Finale: Als PDF ablegen
# pagedown::chrome_print("eh7_cluster_basics.html")
```